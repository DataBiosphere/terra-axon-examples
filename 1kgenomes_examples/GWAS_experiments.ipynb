{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Copyright 2015 Google Inc. All rights reserved. -->\n",
    "\n",
    "<!-- Licensed under the Apache License, Version 2.0 (the \"License\"); -->\n",
    "<!-- you may not use this file except in compliance with the License. -->\n",
    "<!-- You may obtain a copy of the License at -->\n",
    "\n",
    "<!--     http://www.apache.org/licenses/LICENSE-2.0 -->\n",
    "\n",
    "<!-- Unless required by applicable law or agreed to in writing, software -->\n",
    "<!-- distributed under the License is distributed on an \"AS IS\" BASIS, -->\n",
    "<!-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -->\n",
    "<!-- See the License for the specific language governing permissions and -->\n",
    "<!-- limitations under the License. -->\n",
    "\n",
    "# Genome-wide association study (GWAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates conducting a genome-wide association study using the public 1000 Genomes dataset stored in BigQuery.  Specifically we will:\n",
    "* Use the `%%sql` statement to write and execute SQL statements within the notebook\n",
    "* Extract data from BigQuery and create a local dataset that can be manipulated in Python for visualization and further analysis\n",
    "\n",
    "Related Links:\n",
    "* [BigQuery](https://cloud.google.com/bigquery/)\n",
    "* BigQuery [SQL reference](https://cloud.google.com/bigquery/query-reference)\n",
    "* [1,000 Genomes Data Description](http://googlegenomics.readthedocs.org/en/latest/use_cases/discover_public_data/1000_genomes.html)\n",
    "* This notebook is based on the [Google Genomics](https://cloud.google.com/genomics/) BigQuery examples:\n",
    "    * [GWAS Chi-squared Test](https://github.com/googlegenomics/bigquery-examples/blob/master/1000genomes/sql/gwas-pattern-chi-squared-test.sql)\n",
    "    * [GWAS z-test](https://github.com/googlegenomics/bigquery-examples/blob/master/1000genomes/sql/gwas-pattern-two-proportion-z-test.sql)\n",
    "\n",
    "----\n",
    "\n",
    "NOTE:\n",
    "\n",
    "* If you're new to notebooks, or want to check out additional samples, check out the full [list](../) of general notebooks.\n",
    "* For additional Genomics samples, check out the full [list](./) of Genomics notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# In JupyterLab, enable IPython to display matplotlib graphs.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset\n",
    "\n",
    "... more re region setting etc. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_dataset_name = 'GWAS_experiments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!terra resource create bq-dataset --location=US --id $bq_dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Resolves BQ dataset from reference in workspace.\n",
    "'''\n",
    "def get_bq_dataset_from_reference(resource_name):\n",
    "    BQ_CMD_OUTPUT = !terra resolve --name={resource_name}\n",
    "    BQ_DATASET = BQ_CMD_OUTPUT[0]\n",
    "    return BQ_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we'll be identifying variant positions within chromosome 12 that differ significantly between the case and control groups. The case group for the purposes of this notebook will be individuals from the \"EAS\" (East Asian) super population.  Variant data from the 1000 genomes dataset is publicly accessible within BigQuery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_experiments_dataset = get_bq_dataset_from_reference(bq_dataset_name)\n",
    "print(gwas_experiments_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query_config = bigquery.QueryJobConfig(default_dataset=gwas_experiments_dataset)\n",
    "client = bigquery.Client(default_query_job_config=job_query_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying per-call variant positions into variant/non-variant groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tally the reference/alternate allele accounts for *individual* variant positions within chromosome 12. The field `call.genotype` is an integer ranging from `[-1, num_alternate_bases]`. \n",
    "* A value of negative one indicates that the genotype for the call is ambiguous (i.e., a no-call).\n",
    "* A value of zero indicates that the genotype for the call is the same as the reference (i.e., non-variant). \n",
    "* A value of one would indicate that the genotype for the call is the 1st value in the list of alternate bases (likewise for values >1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_table = 'bigquery-public-data.human_genome_variants.1000_genomes_phase_3_variants_20150220'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT reference_name, start_position, reference_bases, \n",
    "alternate_bases[SAFE_OFFSET(0)].alt AS alt_bases, end_position, VT, call_name\n",
    "FROM `{variants_table}` \n",
    "CROSS JOIN UNNEST(call) AS call_name \n",
    "   WHERE\n",
    "      reference_name = '12'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var12_table = f'{gwas_experiments_dataset}.var12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query, passing in the extra configuration.\n",
    "job_query_config = bigquery.QueryJobConfig(destination=var12_table)\n",
    "#job_query_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE  # to overwrite the existing table\n",
    "\n",
    "\n",
    "query_job = client.query(query, job_config=job_query_config)  # Make an API request.\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... ... ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our allele counts match our expectations before moving on. For any given row, the alternate + reference counts should sum to 2 for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = f\"\"\"\n",
    "SELECT reference_name, start_position, end_position, VT[SAFE_OFFSET(0)] as vt, reference_bases, alt_bases, call_name.genotype,\n",
    "(select sum(CAST(num = 0 as int64)) from t.call_name.genotype num) ref_count,\n",
    "(select sum(CAST(num = 1 as int64)) from t.call_name.genotype num) alt_count\n",
    "FROM `{var12}`  t \n",
    "LIMIT 1000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alleles_df = client.query(query1).result().to_dataframe()\n",
    "alleles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning case and control groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join our allele counts with metadata available in the sample info table. We'll use this sample metadata to split the set of genomes into case and control groups based upon the super population group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_info_table= 'bigquery-public-data.human_genome_variants.1000_genomes_sample_info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = f\"\"\"\n",
    "WITH alleles AS (\n",
    "  SELECT reference_name, start_position, end_position, VT[SAFE_OFFSET(0)] as vt, reference_bases, alt_bases, \n",
    "  call_name.genotype, call_name.name as cn,\n",
    "(select sum(CAST(num = 0 as int64)) from t.call_name.genotype num) ref_count,\n",
    "(select sum(CAST(num = 1 as int64)) from t.call_name.genotype num) alt_count\n",
    "FROM `{var12}`  t \n",
    ")\n",
    "SELECT\n",
    "  super_population,\n",
    "  ('EAS' = super_population) AS is_case,\n",
    "  cn,\n",
    "  reference_name,\n",
    "  start_position,\n",
    "  reference_bases,\n",
    "  alt_bases,\n",
    "  end_position,\n",
    "  vt,\n",
    "  ref_count,\n",
    "  alt_count,\n",
    "FROM alleles\n",
    "JOIN `{sample_info_table}` AS samples\n",
    "  ON alleles.cn = samples.sample \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_groups_table = f'{gwas_experiments_dataset}.exp_groups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query, passing in the extra configuration.\n",
    "job_query_config = bigquery.QueryJobConfig(destination=exp_groups_table)\n",
    "\n",
    "query_job = client.query(query2, job_config=job_query_config)  # Make an API request.\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variants table contains a few different types of variant: structural variants (\"SV\"), indels (\"INDEL\") and SNPs (\"SNP\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query3 = f\"\"\"\n",
    "SELECT \n",
    "  vt,\n",
    "  COUNT(*)\n",
    "FROM `{exp_groups_table}`\n",
    "GROUP BY vt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query3)\n",
    "variant_types = query_job.result().to_dataframe()\n",
    "variant_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this experiment, let's limit the variants to only SNPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query4 = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{exp_groups_table}`\n",
    "where vt = 'SNP'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps_table = f'{gwas_experiments_dataset}.snps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query, passing in the extra configuration.\n",
    "job_query_config = bigquery.QueryJobConfig(destination=snps_table)\n",
    "\n",
    "query_job = client.query(query4, job_config=job_query_config)  # Make an API request.\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tallying reference/alternate allele counts for case/control groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've assigned each call set to either the case or the control group, we can tally up the counts of reference and alternate alleles within each of our assigned case/control groups, for each variant position, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = f\"\"\"\n",
    "SELECT  \n",
    "    reference_name,\n",
    "    start_position,\n",
    "    end_position,\n",
    "    reference_bases,\n",
    "    alt_bases,\n",
    "    vt,\n",
    "    SUM(ref_count + alt_count) AS allele_count,\n",
    "    SUM(ref_count) AS ref_count,\n",
    "    SUM(alt_count) AS alt_count,\n",
    "    SUM(IF(TRUE = is_case, \tSAFE_CAST((ref_count + alt_count) AS INT64), 0)) AS case_count,\n",
    "    SUM(IF(FALSE = is_case, SAFE_CAST((ref_count + alt_count) AS INT64), 0)) AS control_count,\n",
    "    SUM(IF(TRUE = is_case, ref_count, 0)) AS case_ref_count,\n",
    "    SUM(IF(TRUE = is_case, alt_count, 0)) AS case_alt_count,\n",
    "    SUM(IF(FALSE = is_case, ref_count, 0)) AS control_ref_count,\n",
    "    SUM(IF(FALSE = is_case, alt_count, 0)) AS control_alt_count,\n",
    "\n",
    "FROM `{snps_table}` \n",
    "GROUP BY\n",
    "    reference_name,\n",
    "    start_position,\n",
    "    end_position,\n",
    "    reference_bases,\n",
    "    alt_bases,\n",
    "    vt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_counts_table = f'{gwas_experiments_dataset}.grouped_counts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query, passing in the extra configuration.\n",
    "job_query_config = bigquery.QueryJobConfig(destination=grouped_counts_table)\n",
    "#job_query_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE  # to overwrite the existing table\n",
    "\n",
    "query_job = client.query(query5, job_config=job_query_config)  # Make an API request.\n",
    "grouped_counts_res = query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in grouped_counts_res.to_dataframe_iterable():\n",
    "    grouped_counts_df = df\n",
    "    break\n",
    "grouped_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, validate that the results are sensical for the group level counts (still per variant position)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the statistical significance at each variant positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantify the statistical significance of each variant position using the Chi-squared test. Furthermore, we can restrict our result set to *only* statistically significant variant positions for this experiment by ranking each position by its statistical signficance (decreasing) and thresholding the results for significance at `p <= 5e-8` (chi-squared score >= 29.7).  \n",
    "(Chi-squared critical value for df=1, p-value=5*10^-8 is 29.71679)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run this query over **all** the variants within chromosome 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query6 = f\"\"\"\n",
    "WITH sres AS (\n",
    "SELECT\n",
    "  reference_name, start_position, end_position, reference_bases, alt_bases, vt,\n",
    "  case_count, control_count, allele_count, ref_count, alt_count,\n",
    "  case_ref_count, case_alt_count, control_ref_count, control_alt_count,\n",
    "  # http://homes.cs.washington.edu/~suinlee/genome560/lecture7.pdf\n",
    "  # https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity\n",
    "  ROUND(\n",
    "    POW(ABS(case_ref_count - (ref_count/allele_count)*case_count) - 0.5,\n",
    "      2)/((ref_count/allele_count)*case_count) +\n",
    "    POW(ABS(control_ref_count - (ref_count/allele_count)*control_count) - 0.5,\n",
    "      2)/((ref_count/allele_count)*control_count) +\n",
    "    POW(ABS(case_alt_count - (alt_count/allele_count)*case_count) - 0.5,\n",
    "      2)/((alt_count/allele_count)*case_count) +\n",
    "    POW(ABS(control_alt_count - (alt_count/allele_count)*control_count) - 0.5,\n",
    "      2)/((alt_count/allele_count)*control_count),\n",
    "    3) AS chi_squared_score\n",
    "FROM `{grouped_counts_table}`\n",
    "WHERE\n",
    "  # For chi-squared, expected counts must be at least 5 for each group\n",
    "  (ref_count/allele_count)*case_count >= 5.0\n",
    "  AND (ref_count/allele_count)*control_count >= 5.0\n",
    "  AND (alt_count/allele_count)*case_count >= 5.0\n",
    "  AND (alt_count/allele_count)*control_count >= 5.0\n",
    ")\n",
    "SELECT * from sres WHERE chi_squared_score >= 29.71679\n",
    "ORDER BY\n",
    "  chi_squared_score DESC,\n",
    "  allele_count DESC\n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_results_table = f'{gwas_experiments_dataset}.stats_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query, passing in the extra configuration.\n",
    "job_query_config = bigquery.QueryJobConfig(destination=stats_results_table)\n",
    "#job_query_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE  # to overwrite the existing table\n",
    "\n",
    "query_job = client.query(query6, job_config=job_query_config)  # Make an API request.\n",
    "stats_res = query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few most significant variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in stats_res.to_dataframe_iterable():\n",
    "    stats_df = df\n",
    "    break\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scroll to the right in the above results to see that the positions deemed significant do in fact have significantly different case/control counts for the alternate/reference bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Chi-squared statistics in BigQuery vs Python vs R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these BigQuery-computed Chi-squared scores to ones calculated via Python's statistical packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2, p, dof, expected = chi2_contingency(np.array([ \n",
    "    [281, 727], # case \n",
    "    [3794, 206]  # control\n",
    "]))\n",
    "\n",
    "print('Python Chi-sq score = %.3f' % chi2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RERUN THIS...\n",
    "Comparing Python statistics to those computed in R separately with the result displayed here:\n",
    "```\n",
    "> chisq.test(rbind(c(220, 352), c(1593, 19)))\n",
    "\n",
    "\tPearson's Chi-squared test with Yates' continuity correction\n",
    "\n",
    "data:  rbind(c(220, 352), c(1593, 19))\n",
    "X-squared = 1086.505, df = 1, p-value < 2.2e-16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see for both the computation in Python and R, that the value matches 1086.505 from BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the GWAS results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, how many statistically significant variant positions did we find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query7 = f\"\"\"\n",
    "SELECT COUNT(*) AS num_significant_snps\n",
    "FROM `{stats_results_table}`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query7)  # Make an API request.\n",
    "df = query_job.result().to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset that is sufficiently small to fit into memory on our instance, so let's pull the top 1000 SNP positions locally. Since we only need a subset of the columns, we can project our data first to remove unneeded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query8 = f\"\"\"\n",
    "SELECT * FROM (\n",
    "  SELECT\n",
    "    reference_name,\n",
    "    start_position,\n",
    "    reference_bases,\n",
    "    alt_bases,\n",
    "    chi_squared_score\n",
    "  FROM `{stats_results_table}`\n",
    "  LIMIT 1000\n",
    ")\n",
    "ORDER BY start_position asc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query8)  # Make an API request.\n",
    "sig_snps_dataset = query_job.result().to_dataframe()\n",
    "sig_snps_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the distribution of significant SNPs along the length of the chromosome. The y-value of the charts indicates the Chi-squared score: larger values are more significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#g = sns.distplot(sig_snps['start'], rug=False, hist=False, kde_kws=dict(bw=0.1))\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(sig_snps_dataset['start_position'], sig_snps_dataset['chi_squared_score'], alpha=0.3, c='red')\n",
    "ax.set_ylabel('Chi-squared score')\n",
    "ax.set_xlabel('SNP position (bp)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in on one region that contains a large number of very significant SNPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(sig_snps_dataset['start_position'], sig_snps_dataset['chi_squared_score'], alpha=0.5, c='red')\n",
    "ax.set_xlim([10.7e7, 12.2e7])\n",
    "# ax.set_xlim([3.3e7, 3.5e7])\n",
    "ax.set_ylabel('Chi-squared score')\n",
    "ax.set_xlabel('SNP position (bp)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrated how to conduct a GWAS experiment using variant data stored within the Google Genomics BigQuery tables, retrieve a local copy of the top results and visualize the data with Python libraries."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "query = \"\"\"\n",
    "\n",
    "SELECT \n",
    "    reference_name,\n",
    "    start,\n",
    "    reference_bases,\n",
    "    alternate_bases,\n",
    "    end,\n",
    "    vt,    \n",
    "    call.call_set_name AS call_set_name,\n",
    "    # 1000 genomes phase 1 data is bi-allelic so there is only ever a single alt\n",
    "    SUM(0 = call.genotype) WITHIN RECORD AS ref_count,\n",
    "    SUM(1 = call.genotype) WITHIN RECORD AS alt_count,\n",
    "FROM\n",
    "  FLATTEN((\n",
    "    SELECT\n",
    "      reference_name,\n",
    "      start,\n",
    "      reference_bases,\n",
    "      alternate_bases,\n",
    "      end,\n",
    "      vt,\n",
    "      call.call_set_name,\n",
    "      call.genotype,\n",
    "    FROM\n",
    "      1000_genomes_phase_3_variants_20150220\n",
    "    WHERE\n",
    "      reference_name = '12' -- i.e., chromosome 12\n",
    "    ),\n",
    "    call)\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# bq.Query(allele_counts, variants_table=variants_table).sample().to_dataframe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "%%sql --module exp_groups\n",
    "\n",
    "SELECT\n",
    "  super_population,\n",
    "  ('EAS' = super_population) AS is_case,\n",
    "  call_set_name,\n",
    "  reference_name,\n",
    "  start,\n",
    "  reference_bases,\n",
    "  alternate_bases,\n",
    "  end,\n",
    "  vt,\n",
    "  ref_count,\n",
    "  alt_count,\n",
    "FROM $allele_counts AS allele_counts\n",
    "JOIN $sample_info_table AS samples\n",
    "  ON allele_counts.call_set_name = samples.sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "bq.Query(exp_groups, allele_counts=allele_counts,\n",
    "                     sample_info_table=sample_info_table,\n",
    "                     variants_table=variants_table).sample().to_dataframe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "bq.Query(snps,\n",
    "         exp_groups=exp_groups,\n",
    "         allele_counts=allele_counts,\n",
    "         sample_info_table=sample_info_table,\n",
    "         variants_table=variants_table).sample().to_dataframe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "%%sql --module grouped_counts\n",
    "\n",
    "SELECT\n",
    "    reference_name,\n",
    "    start,\n",
    "    end,\n",
    "    reference_bases,\n",
    "    alternate_bases,\n",
    "    vt,\n",
    "    SUM(ref_count + alt_count) AS allele_count,\n",
    "    SUM(ref_count) AS ref_count,\n",
    "    SUM(alt_count) AS alt_count,\n",
    "    SUM(IF(TRUE = is_case, INTEGER(ref_count + alt_count), 0)) AS case_count,\n",
    "    SUM(IF(FALSE = is_case, INTEGER(ref_count + alt_count), 0)) AS control_count,\n",
    "    SUM(IF(TRUE = is_case, ref_count, 0)) AS case_ref_count,\n",
    "    SUM(IF(TRUE = is_case, alt_count, 0)) AS case_alt_count,\n",
    "    SUM(IF(FALSE = is_case, ref_count, 0)) AS control_ref_count,\n",
    "    SUM(IF(FALSE = is_case, alt_count, 0)) AS control_alt_count,\n",
    "FROM $snps\n",
    "GROUP BY\n",
    "    reference_name,\n",
    "    start,\n",
    "    end,\n",
    "    reference_bases,\n",
    "    alternate_bases,\n",
    "    vt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "bq.Query(grouped_counts,\n",
    "         snps=snps,\n",
    "         exp_groups=exp_groups,\n",
    "         allele_counts=allele_counts,\n",
    "         sample_info_table=sample_info_table,\n",
    "         variants_table=variants_table).sample().to_dataframe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "%%sql --module results\n",
    "\n",
    "SELECT\n",
    "  reference_name,\n",
    "  start,\n",
    "  end,\n",
    "  reference_bases,\n",
    "  alternate_bases,\n",
    "  vt,\n",
    "  case_count,\n",
    "  control_count,\n",
    "  allele_count,\n",
    "  ref_count,\n",
    "  alt_count,\n",
    "  case_ref_count,\n",
    "  case_alt_count,\n",
    "  control_ref_count,\n",
    "  control_alt_count,\n",
    "  # http://homes.cs.washington.edu/~suinlee/genome560/lecture7.pdf\n",
    "  # https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity\n",
    "  ROUND(\n",
    "    POW(ABS(case_ref_count - (ref_count/allele_count)*case_count) - 0.5,\n",
    "      2)/((ref_count/allele_count)*case_count) +\n",
    "    POW(ABS(control_ref_count - (ref_count/allele_count)*control_count) - 0.5,\n",
    "      2)/((ref_count/allele_count)*control_count) +\n",
    "    POW(ABS(case_alt_count - (alt_count/allele_count)*case_count) - 0.5,\n",
    "      2)/((alt_count/allele_count)*case_count) +\n",
    "    POW(ABS(control_alt_count - (alt_count/allele_count)*control_count) - 0.5,\n",
    "      2)/((alt_count/allele_count)*control_count),\n",
    "    3) AS chi_squared_score\n",
    "FROM $grouped_counts\n",
    "WHERE\n",
    "  # For chi-squared, expected counts must be at least 5 for each group\n",
    "  (ref_count/allele_count)*case_count >= 5.0\n",
    "  AND (ref_count/allele_count)*control_count >= 5.0\n",
    "  AND (alt_count/allele_count)*case_count >= 5.0\n",
    "  AND (alt_count/allele_count)*control_count >= 5.0\n",
    "HAVING\n",
    "  # Chi-squared critical value for df=1, p-value=5*10^-8 is 29.71679\n",
    "  chi_squared_score >= 29.71679\n",
    "ORDER BY\n",
    "  chi_squared_score DESC,\n",
    "  allele_count DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bq.Query(results,\n",
    "         grouped_counts=grouped_counts,\n",
    "         snps=snps,\n",
    "         exp_groups=exp_groups,\n",
    "         allele_counts=allele_counts,\n",
    "         sample_info_table=sample_info_table,\n",
    "         variants_table=variants_table).sample().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# #g = sns.distplot(sig_snps['start'], rug=False, hist=False, kde_kws=dict(bw=0.1))\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(sig_snps_dataset`['start'], sig_snps_dataset['chi_squared_score'], alpha=0.3, c='red')\n",
    "# ax.set_ylabel('Chi-squared score')\n",
    "# ax.set_xlabel('SNP position (bp)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(sig_snps['start'], sig_snps['chi_squared_score'], alpha=0.5, c='red')\n",
    "# ax.set_xlim([3.3e7, 3.5e7])\n",
    "# ax.set_ylabel('Chi-squared score')\n",
    "# ax.set_xlabel('SNP position (bp)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
