{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Genome-wide association study (GWAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates conducting a genome-wide association study using the public 1000 Genomes dataset stored in BigQuery.  \n",
    "\n",
    "Related Links:\n",
    "* [BigQuery](https://cloud.google.com/bigquery/)\n",
    "* BigQuery [SQL reference](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax)\n",
    "* [1,000 Genomes summary](https://www.internationalgenome.org/1000-genomes-summary/)\n",
    "* [Verily Workbench documentation](https://support.workbench.verily.com/docs/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we'll be identifying variant positions within chromosome 12 that differ significantly between the case and control groups. The case group for the purposes of this notebook will be individuals from the \"EAS\" (East Asian) super population.  Variant data from the 1000 genomes dataset is publicly accessible within BigQuery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to be run using [Verily Workbench](https://workbench.verily.com/).   \n",
    "It's most straighforward to run it from a Workbench [workspace's](https://support.workbench.verily.com/docs/technical_reference/workspaces/) [JupyterLab cloud environment](xxx).\n",
    "\n",
    "However, you can also run the notebook locally. To do so, you will need to have **first [installed and authorized the Workbench CLI](https://support.workbench.verily.com/docs/technical_reference/cli/cli_install_and_run/), and [created a workspace](https://support.workbench.verily.com/docs/technical_reference/workspaces/workspace_operations/)**.\n",
    "\n",
    "**TODO**: notebook approximate costs (including BQ query costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, do some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# In JupyterLab, enable IPython to display matplotlib graphs.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define a utility function to resolve the BQ dataset name from its reference name in the workspace.\n",
    "It uses the Workbench CLI.\n",
    "'''\n",
    "def get_bq_dataset_from_reference(resource_name):\n",
    "    BQ_CMD_OUTPUT = !wb resolve --name={resource_name}\n",
    "    BQ_DATASET = BQ_CMD_OUTPUT[0]\n",
    "    return BQ_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a BigQuery dataset for our explorations\n",
    "\n",
    "Next, we'll create a workspace-managed ([\"controlled\"](https://support.workbench.verily.com/docs/technical_reference/data_resources/#referenced-vs-workspace-controlled-data-resources)) BigQuery dataset to use for this example. \n",
    "\n",
    "This dataset needs to be in the `US` region.  That is because the `bigquery-public-data.human_genome_variants` dataset, which we'll use, is in the `US` region. To save the results of queries over that dataset, the workspace dataset must be in the same region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_dataset_name = 'GWAS_experiments'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command to create the dataset.  You'll see an ignorable error if you run this more than once.\n",
    "\n",
    "**Note**: if you're not running this notebook in the context of a Verily Workbench workspace, ensure first that the `wb` utility is set to the workspace in which you want to create the dataset. You can check this by running `wb workspace describe`. If need be, you can run `wb workspace set --id=<your_workspace_id>`to set the workspace.  \n",
    "(If you're running the notebook from a workspace cloud environment, `wb` will be set already to use that workspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wb resource create bq-dataset --location=US --id $bq_dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get the full BQ dataset name from the reference name. It should have the form `<project_id>.<dataset_name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_experiments_dataset = get_bq_dataset_from_reference(bq_dataset_name)\n",
    "print(gwas_experiments_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a BQ client object. We'll use this for our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_query_config = bigquery.QueryJobConfig(default_dataset=gwas_experiments_dataset)\n",
    "client = bigquery.Client(default_query_job_config=job_query_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're set up to run queries and create new tables in the workspace dataset we created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying per-call variant positions into variant/non-variant groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tally the reference/alternate allele accounts for *individual* variant positions within chromosome 12. The field `call.genotype` is an integer ranging from `[-1, num_alternate_bases]`. \n",
    "* A value of negative one indicates that the genotype for the call is ambiguous (i.e., a no-call).\n",
    "* A value of zero indicates that the genotype for the call is the same as the reference (i.e., non-variant). \n",
    "* A value of one would indicate that the genotype for the call is the 1st value in the list of alternate bases (likewise for values >1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_table = 'bigquery-public-data.human_genome_variants.1000_genomes_phase_3_variants_20150220'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query below, we're setting a destination table (the value of `var12_table`) in the workspace dataset. (For most of the queries in this notebook, the result sets are way too large to hold in-memory on the notebook server).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT reference_name, start_position, reference_bases,\n",
    "alternate_bases[SAFE_OFFSET(0)].alt AS alt_bases, end_position, VT, call_info\n",
    "FROM `{variants_table}`\n",
    "CROSS JOIN UNNEST(call) AS call_info\n",
    "   WHERE\n",
    "      reference_name = '12'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var12_table = f'{gwas_experiments_dataset}.var12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run the actual query, which will create a new 'var12' table in the workspace dataset with the results. In this case, we don't need a handle to the returned `RowIterator`, but we'll show how to use that in subsequent queries.\n",
    "\n",
    "If the given destination table already exists, the query will fail.  If you'd like to override this and overwrite the existing table with the new query results, uncomment the line that sets `bigquery.WriteDisposition.WRITE_TRUNCATE`. \n",
    "\n",
    "You can similarly add this `WRITE_TRUNCATE` config for any of the queries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query, passing in the destination table config.\n",
    "job_query_config = bigquery.QueryJobConfig(destination=var12_table)\n",
    "#job_query_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE  # uncomment to overwrite the existing table\n",
    "\n",
    "query_job = client.query(query, job_config=job_query_config)  # Make an API request.\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our allele counts match our expectations before moving on. For any given row, the alternate + reference counts should sum to 2 for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = f\"\"\"\n",
    "SELECT reference_name, start_position, end_position, VT[SAFE_OFFSET(0)] as vt, reference_bases, alt_bases, call_info.genotype,\n",
    "(select sum(CAST(num = 0 as int64)) from t.call_info.genotype num) ref_count,\n",
    "(select sum(CAST(num = 1 as int64)) from t.call_info.genotype num) alt_count\n",
    "FROM `{var12_table}`  t\n",
    "LIMIT 1000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alleles_df = client.query(query1).result().to_dataframe()\n",
    "alleles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning case and control groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join our allele counts with metadata available in the sample info table. We'll use this sample metadata to split the set of genomes into case and control groups based upon the super population group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_info_table= 'bigquery-public-data.human_genome_variants.1000_genomes_sample_info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = f\"\"\"\n",
    "WITH alleles AS (\n",
    "  SELECT reference_name, start_position, end_position, VT[SAFE_OFFSET(0)] as vt, reference_bases, alt_bases,\n",
    "  call_info.genotype, call_info.name as cn,\n",
    "(select sum(CAST(num = 0 as int64)) from t.call_info.genotype num) ref_count,\n",
    "(select sum(CAST(num = 1 as int64)) from t.call_info.genotype num) alt_count\n",
    "FROM `{var12_table}`  t\n",
    ")\n",
    "SELECT\n",
    "  super_population,\n",
    "  ('EAS' = super_population) AS is_case,\n",
    "  cn,\n",
    "  reference_name,\n",
    "  start_position,\n",
    "  reference_bases,\n",
    "  alt_bases,\n",
    "  end_position,\n",
    "  vt,\n",
    "  ref_count,\n",
    "  alt_count,\n",
    "FROM alleles\n",
    "JOIN `{sample_info_table}` AS samples\n",
    "  ON alleles.cn = samples.sample\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_groups_table = f'{gwas_experiments_dataset}.exp_groups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query, passing in the destination table config.\n",
    "job_query_config = bigquery.QueryJobConfig(destination=exp_groups_table)\n",
    "\n",
    "query_job = client.query(query2, job_config=job_query_config)\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variants table contains a few different types of variant: structural variants (\"SV\"), indels (\"INDEL\") and SNPs (\"SNP\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query3 = f\"\"\"\n",
    "SELECT\n",
    "  vt,\n",
    "  COUNT(*)\n",
    "FROM `{exp_groups_table}`\n",
    "GROUP BY vt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small result, so we can save it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query3)\n",
    "variant_types = query_job.result().to_dataframe()\n",
    "variant_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this experiment, let's limit the variants to only SNPs. To keep things simple, we'll create a new dedicated table for just the SNP variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query4 = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{exp_groups_table}`\n",
    "where vt = 'SNP'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps_table = f'{gwas_experiments_dataset}.snps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query, passing in the extra configuration.\n",
    "job_query_config = bigquery.QueryJobConfig(destination=snps_table)\n",
    "\n",
    "query_job = client.query(query4, job_config=job_query_config)\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tallying reference/alternate allele counts for case/control groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've assigned each call set to either the case or the control group, we can tally up the counts of reference and alternate alleles within each of our assigned case/control groups, for each variant position, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = f\"\"\"\n",
    "SELECT\n",
    "    reference_name,\n",
    "    start_position,\n",
    "    end_position,\n",
    "    reference_bases,\n",
    "    alt_bases,\n",
    "    vt,\n",
    "    SUM(ref_count + alt_count) AS allele_count,\n",
    "    SUM(ref_count) AS ref_count,\n",
    "    SUM(alt_count) AS alt_count,\n",
    "    SUM(IF(TRUE = is_case, \tSAFE_CAST((ref_count + alt_count) AS INT64), 0)) AS case_count,\n",
    "    SUM(IF(FALSE = is_case, SAFE_CAST((ref_count + alt_count) AS INT64), 0)) AS control_count,\n",
    "    SUM(IF(TRUE = is_case, ref_count, 0)) AS case_ref_count,\n",
    "    SUM(IF(TRUE = is_case, alt_count, 0)) AS case_alt_count,\n",
    "    SUM(IF(FALSE = is_case, ref_count, 0)) AS control_ref_count,\n",
    "    SUM(IF(FALSE = is_case, alt_count, 0)) AS control_alt_count,\n",
    "\n",
    "FROM `{snps_table}`\n",
    "GROUP BY\n",
    "    reference_name,\n",
    "    start_position,\n",
    "    end_position,\n",
    "    reference_bases,\n",
    "    alt_bases,\n",
    "    vt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_counts_table = f'{gwas_experiments_dataset}.grouped_counts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this query, we'll grab a handle to the returned query result (a `RowIterator`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query, passing in the extra configuration.\n",
    "job_query_config = bigquery.QueryJobConfig(destination=grouped_counts_table)\n",
    "#job_query_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE  # to overwrite the existing table\n",
    "\n",
    "query_job = client.query(query5, job_config=job_query_config)  # Make an API request.\n",
    "grouped_counts_res = query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the query result iterator to write a subset of results to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in grouped_counts_res.to_dataframe_iterable():\n",
    "    grouped_counts_df = df\n",
    "    break\n",
    "grouped_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, validate that the results are sensical for the group level counts (still per variant position)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the statistical significance at each variant positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantify the statistical significance of each variant position using the Chi-squared test. Furthermore, we can restrict our result set to *only* statistically significant variant positions for this experiment by ranking each position by its statistical signficance (decreasing) and thresholding the results for significance at `p <= 5e-8` (chi-squared score >= 29.7).  \n",
    "(Chi-squared critical value for df=1, p-value=5*10^-8 is 29.71679)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run this query over **all** the variants within chromosome 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query6 = f\"\"\"\n",
    "WITH sres AS (\n",
    "SELECT\n",
    "  reference_name, start_position, end_position, reference_bases, alt_bases, vt,\n",
    "  case_count, control_count, allele_count, ref_count, alt_count,\n",
    "  case_ref_count, case_alt_count, control_ref_count, control_alt_count,\n",
    "  # http://homes.cs.washington.edu/~suinlee/genome560/lecture7.pdf\n",
    "  # https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity\n",
    "  ROUND(\n",
    "    POW(ABS(case_ref_count - (ref_count/allele_count)*case_count) - 0.5,\n",
    "      2)/((ref_count/allele_count)*case_count) +\n",
    "    POW(ABS(control_ref_count - (ref_count/allele_count)*control_count) - 0.5,\n",
    "      2)/((ref_count/allele_count)*control_count) +\n",
    "    POW(ABS(case_alt_count - (alt_count/allele_count)*case_count) - 0.5,\n",
    "      2)/((alt_count/allele_count)*case_count) +\n",
    "    POW(ABS(control_alt_count - (alt_count/allele_count)*control_count) - 0.5,\n",
    "      2)/((alt_count/allele_count)*control_count),\n",
    "    3) AS chi_squared_score\n",
    "FROM `{grouped_counts_table}`\n",
    "WHERE\n",
    "  # For chi-squared, expected counts must be at least 5 for each group\n",
    "  (ref_count/allele_count)*case_count >= 5.0\n",
    "  AND (ref_count/allele_count)*control_count >= 5.0\n",
    "  AND (alt_count/allele_count)*case_count >= 5.0\n",
    "  AND (alt_count/allele_count)*control_count >= 5.0\n",
    ")\n",
    "SELECT * from sres WHERE chi_squared_score >= 29.71679\n",
    "ORDER BY\n",
    "  chi_squared_score DESC,\n",
    "  allele_count DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_results_table = f'{gwas_experiments_dataset}.stats_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the query, passing in the extra configuration.\n",
    "job_query_config = bigquery.QueryJobConfig(destination=stats_results_table)\n",
    "#job_query_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE  # to overwrite the existing table\n",
    "\n",
    "query_job = client.query(query6, job_config=job_query_config)  # Make an API request.\n",
    "stats_res = query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few most significant variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in stats_res.to_dataframe_iterable():\n",
    "    stats_df = df\n",
    "    break\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scroll to the right in the above results to see that the positions deemed significant do in fact have significantly different case/control counts for the alternate/reference bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Chi-squared statistics in BigQuery vs Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these BigQuery-computed Chi-squared scores to ones calculated via Python's statistical packages, for the first row in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2, p, dof, expected = chi2_contingency(np.array([\n",
    "    [281, 727], # case\n",
    "    [3794, 206]  # control\n",
    "]))\n",
    "\n",
    "print('Python Chi-sq score = %.3f' % chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for the computation in Python that the value matches that computed by BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the GWAS results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, how many statistically significant variant positions did we find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query7 = f\"\"\"\n",
    "SELECT COUNT(*) AS num_significant_snps\n",
    "FROM `{stats_results_table}`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query7)\n",
    "df = query_job.result().to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull in the top 1000 SNP positions to local memory. Since we only need a subset of the columns, we can project our data first to remove unneeded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query8 = f\"\"\"\n",
    "SELECT * FROM (\n",
    "  SELECT\n",
    "    reference_name,\n",
    "    start_position,\n",
    "    reference_bases,\n",
    "    alt_bases,\n",
    "    chi_squared_score\n",
    "  FROM `{stats_results_table}`\n",
    "  LIMIT 1000\n",
    ")\n",
    "ORDER BY start_position asc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query8)  # Make an API request.\n",
    "sig_snps_dataset = query_job.result().to_dataframe()\n",
    "sig_snps_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the distribution of significant SNPs along the length of the chromosome. The y-value of the charts indicates the Chi-squared score: larger values are more significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#g = sns.distplot(sig_snps['start'], rug=False, hist=False, kde_kws=dict(bw=0.1))\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(sig_snps_dataset['start_position'], sig_snps_dataset['chi_squared_score'], alpha=0.3, c='red')\n",
    "ax.set_ylabel('Chi-squared score')\n",
    "ax.set_xlabel('SNP position (bp)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in on one region that contains a large number of very significant SNPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(sig_snps_dataset['start_position'], sig_snps_dataset['chi_squared_score'], alpha=0.5, c='red')\n",
    "ax.set_xlim([10.7e7, 12.2e7])\n",
    "# ax.set_xlim([3.3e7, 3.5e7])\n",
    "ax.set_ylabel('Chi-squared score')\n",
    "ax.set_xlabel('SNP position (bp)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrated how to conduct a GWAS experiment using variant data stored within the Google Genomics BigQuery tables, retrieve a local copy of the top results and visualize the data with Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "You can delete the dataset you created (`GWAS_experiments`) if you are done with the tables that you created.\n",
    "\n",
    "You can do this via the `wb` command-line utility like this:  \n",
    "`wb resource delete --id GWAS_experiments`\n",
    "\n",
    "You can also delete the dataset via the [Verily Workbench UI](https://support.workbench.verily.com/docs/technical_reference/data_resources/data_resources_operations/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright 2024 Verily Life Sciences LLC\n",
    "\n",
    "Use of this source code is governed by a BSD-style  \n",
    "license that can be found in the LICENSE file or at  \n",
    "https://developers.google.com/open-source/licenses/bsd"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
